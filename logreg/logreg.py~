# Austin Anderson
# Machine Learning
# logreg.py
# Script runs online logistic regression calculations using steepest gradient descent to determine if a document refers to hockey or baseball. 
# Script supports unregulated or regulated updates.
# This sciprt supports variable step sizes in its optimization calculation.
# The end of the script plots results and prints the most and least important words for the two classifications. 

import random
import operator # For sort command
from numpy import zeros, sign, dot, inner, add, spacing # dot for dot product calculation in sg_update, add for adding vectors, and spacing for machine precision offset
from numpy import linalg as LA # LA is used in 
from math import exp, log, log10 # log10 used for convergence plotting
from collections import defaultdict # 
import matplotlib.pyplot as plt # Just used to make plots

import argparse

kSEED = 1701 # AA COMMENT: seed the random number gen
kBIAS = "BIAS_CONSTANT" # Used to check that BIAS_CONSTANT is not in the paper

random.seed(kSEED)

# Sigmoid calculates the logistic function with saturation
def sigmoid(score, threshold=20.0):
    """
    Prevent overflow of exp by capping activation at 20.

    :param score: A real valued number to convert into a number between 0 and 1
    """

    if abs(score) > threshold:
        score = threshold * sign(score)

    activation = exp(score)
    return activation / (1.0 + activation)

# The class Example just makes a structure and counts up words 
# in the feature set
# includes known labels
# Chnaged nonzero to give a full vector of len(vocab) that just 
# has a 1 if the entry is nonzero and a 0 if it's 0
class Example:
    """
    Class to represent a logistic regression example
    """
    def __init__(self, label, words, vocab, df):
        """
        Create a new example

        :param label: The label (0 / 1) of the example
        :param words: The words in a list of "word:count" format
        :param vocab: The vocabulary to use as features (list)
        """
        #self.nonzero = {} # AA CHANGE
        self.nonzero = zeros(len(vocab))
        self.y = label
        self.x = zeros(len(vocab))
        for word, count in [x.split(":") for x in words]:
            if word in vocab:
                assert word != kBIAS, "Bias can't actually appear in document" 
                self.x[vocab.index(word)] += float(count)
                #self.nonzero[vocab.index(word)] = word # AA CHANGE
                self.nonzero[vocab.index(word)] = 1
        self.x[0] = 1
        self.nonzero[0] = 1

# The meat of the script
class LogReg:
    def __init__(self, num_features, mu, step): # changed away from the lambda call to just assign step. I could see how the update call could be substituted, but caused issues when trying to simply pass and argument
        """
        Create a logistic regression classifier

        :param num_features: The number of features (including bias)
        :param mu: Regularization parameter
        :param step: A function that takes the iteration as an argument (the default is a constant value)
        """
        # Creates structure for log reg
        # Sets up weights beta, mu, the regulation parameter and step size 

        self.beta = zeros(num_features)
        self.mu = mu
        self.step = step
        self.last_update = zeros(num_features) # Edited last_update to now hold the running count vector of when a Beta value was last updated for the regularization step

        assert self.mu >= 0, "Regularization parameter must be non-negative"

    def progress(self, examples):
        """
        Given a set of examples, compute the probability and accuracy

        :param examples: The dataset to score
        :return: A tuple of (log probability, accuracy)
        """
    # This calculates the logprob and accuracy given
    # your current weights, input data and labels 
    # the first portion calculates the probability p(y|x,B)
    # the second portion checks if it's right by looking at the sigmoid
    # the sigmoid is bounded by 0,1 so if we're stricly less than half 
    # way, it's counted as correct and we incrament the number right
    # then the accuracy is just how many we got right over the total

        logprob = 0.0
        num_right = 0
        for ii in examples:
            p = sigmoid(self.beta.dot(ii.x))
            if ii.y == 1:
                logprob += log(p) 

            else:
                logprob += log(1.0 - p)

            # Get accuracy
            if abs(ii.y - p) < 0.5:
                num_right += 1

        return logprob, float(num_right) / float(len(examples))

    def sg_update(self, train_example, iteration, use_tfidf=False):
        """
        Compute a stochastic gradient update to improve the log likelihood.

        :param train_example: The example to take the gradient with respect to
        :param iteration: The current iteration (an integer)
        :param use_tfidf: A boolean to switch between the raw data and the tfidf representation
        :return: Return the new value of the regression coefficients
        """

        # AA COMMENT: This first section calculates the basic unregularized update
        # calculate pi_i
	# This version runs the step_update function to update the step size. This line can be commented to return to fixed, user-specified
        self.step = step_update(self,iteration)
	# If it's the first itteration, initialize the last_update vector to all zeros
        if(iteration==0):
            self.last_update = zeros(len(train_example.x))
	# Calculate the Beta,feature dot product
        dot_pi = dot(self.beta,train_example.x) 
	# Calculate the exponential
        exp_pi = exp(dot_pi)
        pi_i = exp_pi/(1+exp_pi)
	# Update the betas
        for ii in range(0,len(self.beta)):
            self.beta[ii] = self.beta[ii]+self.step*(train_example.y-pi_i)*train_example.x[ii]

	# AA COMMENT: The regularization update
        # Ok, so here, we have self.nonzero saying which index is non-zero
        # so all we have to do is update the values of last_update that 
        # are nonzero. If it's non-zero we update it and reset its last_update count
	# otherwise we update the last_update count and move on
        
        for jj in range(0,len(train_example.x)):
            if(train_example.x[jj]!=0):
                self.beta[jj] = self.beta[jj]*pow((1-2*self.step*self.mu),self.last_update[jj]+1)

            if(train_example.nonzero[jj] == 1):
                self.last_update[jj] = 0
            else:
                self.last_update[jj] += 1
            
        return self.beta


def read_dataset(positive, negative, vocab, test_proportion=.1):
    """
    Reads in a text dataset with a given vocabulary

    :param positive: Positive examples
    :param negative: Negative examples
    :param vocab: A list of vocabulary words
    :param test_proprotion: How much of the data should be reserved for test
    """
    df = [float(x.split("\t")[1]) for x in open(vocab, 'r') if '\t' in x]
    vocab = [x.split("\t")[0] for x in open(vocab, 'r') if '\t' in x]
    assert vocab[0] == kBIAS, \
        "First vocab word must be bias term (was %s)" % vocab[0]

    train = []
    test = []
    for label, input in [(1, positive), (0, negative)]:
        for line in open(input):
            ex = Example(label, line.split(), vocab, df)
            if random.random() <= test_proportion:
                test.append(ex)
            else:
                train.append(ex)

    # Shuffle the data so that we don't have order effects
    random.shuffle(train)
    random.shuffle(test)

    return train, test, vocab
# 
def step_update(self,iteration):
    # AA COMMENT: Now this function updates the step-size based on interation. 
    # The function used is (8.83) in the Murphy text. In that equation k was 
    # bounded by (0.5,1]. A value of k = 0.1 was found to work better, though. 
    # The updated step size is returned. Generally this algorithm slows down 
    # early iterations and adds hysteresis. 
    k = 0.1
    step_update = pow(self.step+iteration,-1*k)
    return step_update

if __name__ == "__main__":
    argparser = argparse.ArgumentParser()
    argparser.add_argument("--mu", help="Weight of L2 regression",
                           type=float, default=0.0, required=False)
    argparser.add_argument("--step", help="Initial SG step size",
                           type=float, default=0.1, required=False)
    argparser.add_argument("--positive", help="Positive class",
                           type=str, default="../data/hockey_baseball/positive", required=False)
    argparser.add_argument("--negative", help="Negative class",
                           type=str, default="../data/hockey_baseball/negative", required=False)
    argparser.add_argument("--vocab", help="Vocabulary that can be features",
                           type=str, default="../data/hockey_baseball/vocab", required=False)
    argparser.add_argument("--passes", help="Number of passes through train",
                           type=int, default=1, required=False)

    args = argparser.parse_args()
    train, test, vocab = read_dataset(args.positive, args.negative, args.vocab)

    print("Read in %i train and %i test" % (len(train), len(test)))

    # Initialize model
    lr = LogReg(len(vocab), args.mu, args.step)

    # Iterations, added vectors for plotting results
    update_number = 0
    t_vec = zeros(len(train))
    tp_vec = zeros(len(train))
    hp_vec = zeros(len(train))
    ta_vec = zeros(len(train))
    ha_vec = zeros(len(train))
    bn_vec = zeros(len(train))
    for pp in xrange(args.passes):
        for ii in train:
            beta_vec = lr.sg_update(ii, update_number)
	    train_lp, train_acc = lr.progress(train)
            ho_lp, ho_acc = lr.progress(test)
	    t_vec[update_number] = update_number
	    tp_vec[update_number] = train_lp
	    ta_vec[update_number] = train_acc
	    hp_vec[update_number] = ho_lp
	    ha_vec[update_number] = ho_acc
	    bn_vec[update_number] = LA.norm(beta_vec) # norm calculation for determining rough metric for convergence
            update_number += 1	
	    # Now just prints out percent done and plots results 
            if update_number % 100 == 1:
		per_done = float(100*update_number)/float(len(train))
		#per_done = len(train)
                print("Percent done: %i\r" %
                      int(round(per_done)))

    bd_vec = zeros(len(bn_vec)-1) # calculate difference in beta vector norms from iteration to iteragion to estimate convergence 
    for jj in range(len(bn_vec)-1):
	bd_vec[jj] = 10*log10(abs(bn_vec[jj+1]-bn_vec[jj])+spacing(1))

    dict_hi = {}
    dict_lo = {} # this code is all for determining most and least important words in the classification
    for kk in range(len(vocab)):
	dict_hi[vocab[kk]] = lr.beta[kk]
	dict_lo[vocab[kk]] = abs(lr.beta[kk])
    # Determines this by sorting the dictionary and finding the highly positive, highly negative, and lowest amplitude words 
    bas_x = sorted(dict_hi.items(), key=operator.itemgetter(1),reverse = True)
    hoc_x = sorted(dict_hi.items(), key=operator.itemgetter(1))
    lo_x = sorted(dict_lo.items(), key=operator.itemgetter(1))
    v_rng = 5
    x_bas = range(v_rng)
    x_hoc = range(v_rng)
    x_low = range(2*v_rng)
    for ll in range(v_rng):
	x_bas[ll] = bas_x[ll][0]
	x_hoc[ll] = hoc_x[ll][0]

    for mm in range(2*v_rng):
	x_low[mm] = lo_x[mm][0]

    # Print vocab results 
    print x_bas # Baseball
    print x_hoc # Hockey
    print x_low # Low impact for both 
    # Plot results 
    plt.figure(1)
    plt.subplot(2,1,1)
    plt.plot(t_vec,ta_vec,t_vec,ha_vec)
    #plt.xlabel('Iteration Number')
    plt.ylabel('Percent Accuracy')
    plt.title('Accuracy')
    plt.legend(["Baseball","Hockey"])
    plt.grid(True)
    plt.subplot(2,1,2)
    plt.plot(t_vec,tp_vec,t_vec,hp_vec)
    plt.xlabel('Iteration Number')
    plt.ylabel('Loglikelihood')
    plt.title('Probability')
    plt.legend(["Baseball","Hockey"])
    plt.grid(True)
    plt.figure(2)
    plt.plot(bd_vec)
    plt.xlabel('Iteration Number')
    plt.ylabel('Norm Diff (dB)')
    plt.title('Change in Beta')
    plt.show()



